---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

[PILOT/COPILOT - TEXT IN SQUARE BRACKETS IS HERE FOR GUIDANCE. COPILOT PLEASE DELETE BEFORE KNITTING THE FINAL REPORT]

# Report Details

[PILOT/COPILOT ENTER RELEVANT REPORT DETAILS HERE]

```{r}
articleID <- NA # "CARPS_EXT_29-4-2015"
reportType <- NA # specify whether this is the 'pilot' report or 'final' report
pilotNames <- NA # "Melissa Mesinas"
copilotNames <- NA # # "Jinxiao Zhang"
pilotTTC <- NA # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- NA # insert the co-pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- NA # as.Date("11/02/18", format = "%m/%d/%y")
copilotStartDate <- NA # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- NA # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 

[PILOT/COPILOT write a brief summary of the methods underlying the target outcomes written in your own words]

The authors videotaped M.B.A students giving spoken elevator pitches to their potential employers. Evaluators then watched, listened to, or read transcripts of the videos. Eighteen M.B.A students at the University of Chicago Booth School of Business (mean age = 28.2 years, SD = 2.07; 11 males) responded to the request of serving as job candidates in exchange for a small gift card. Then, 162 people (mean age = 36.86 years, SD = 15.01; 80 males) were recruited at the Museum of Science and Industry in Chicago to evaluate the candidates in exchange for a food item. There were three conditions in which evaluators (1) watched, (2) listened, or (3) read) the pitch. Three people evaluate each of the 18 candidates in each of the three conditions (i.e. slightly more than 50 evaluators per condition). Fifty participants per condition yields 80& power to detect a medium-sized effect. 

Those in the video condition watched and listened to a candidate's spoken pitch, those in the audio condition only listened to a spoken pitch, those in the transcript condition only read a transcribed pitch. Each evaluator only observed only one candidate's pitch in one medium. Evaluators completed a survey after and answered three questions about the candidate's intellect: They rated (1) how competent the candidate seemed compared with an average candidate for an M.B.A-level position (-5 = much less competent, 5 = much more competent), (2) how thoughtful the candidate seemed compared with the average candidate for an M.B.A.-level position (-5 = much less thoughtful, 5 = much more thoughtful) and (3) how intelligent the candidate seemed compared with an average candidate for an M.B.A.-level position (-5 = much less intelligent, 5 = much more intelligent). Evaluators then reported their general impressions of the candidate, such as how much they liked the candidate ( 0 = did not like at all, 10 = extremely liked), how positive their overall impression of the candidate was (0 = not at all positive, 10 = extremely positive) and how negative their overall impression of the candidate was (0 = not at all negative, 10 = extremely negative, reverse-scored). Lastly, participants rated how likely they would be to hire the candidate for a job (0 = not at all likely, 10 = extremely likedly). The authors averaged the ratings of intelligence, competence, and thoughtfulness to crate a composite measure of intellect (alpha = .91) and the ratings of liking, positive impression, and negative impression to create a composite score of general impressions (alpha = .89).  
------

#### Target outcomes: 

[PILOT copy and paste the target outcomes identified in targetOutcomes.md]  

> Hypothetical  employers’  evaluations. As  predicted, evaluators’  beliefs  about  job  candidates’  intellect—their  competence, thoughtfulness, and intelligence—depended on  the  communication  medium, F(2,  157)  =  10.81, p < .01, η2 =  .12.  As  indicated  by  the  standardized  scores  shown  in  Figure  1,  evaluators  who  heard  pitches  rated the  candidates’  intellect  more  highly  (M =  0.91, SD =  1.79) than did evaluators who read transcripts of pitches (M = −0.70, SD = 2.81), t(157) = 3.79, p < .01, 95% confidence  interval  (CI)  of  the  difference  =  [0.70,  2.51], d = 0.60.  Evaluators  who  watched  pitches  did  not  evaluate the candidates’ intellect (M = 1.09, SD = 1.80) differently than evaluators who listened to pitches, t(157) < 1. Simply adding more individuating information about a candidate through visual  cues,  such  as  physical  appearance  and nonverbal  mannerisms,  had  no  measurable  impact  on  evaluations of the candidate’s mind. Candidates’ intellect was conveyed primarily through their voice.

> Perhaps more important, evaluators who heard pitches also  reported  more  favorable  impressions  of  the  candidates—liked the candidates more and had more positive and  less  negative  impressions  of  the candidates—than  did evaluators who read pitches (M = 5.69, SD = 1.96, vs. M = 4.78, SD = 2.64), t(159) = 2.16, p = .03, 95% CI of the difference = [0.02, 1.80], d = 0.34 (see Fig. 1). Evaluators who heard pitches also reported being significantly more likely to hire the candidates (M = 4.34, SD = 2.26) than did evaluators who read exactly the same pitches (M = 3.06, SD = 3.15), t(156) = 2.49, p = .01, 95% CI of the difference = [0.22, 2.34], d = 0.40 (see Fig. 1). These results again did not appear to stem simply from having more individuating information about the candidates in the audio condition, because evaluators who watched pitches did not report more favorable impressions (M = 5.98, SD = 1.91) or an increased likelihood of hiring the candidates (M = 4.46, SD = 2.43) compared with evaluators
who heard pitches, ts < 1.

------

[PILOT/COPILOT DO NOT CHANGE THE CODE IN THE CHUNK BELOW]  

```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object

[PILOT/COPILOT Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
```

[PILOT/COPILOT DO NOT MAKE CHANGES TO THE CODE CHUNK BELOW]

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
data <- read_sav("/Users/melissamesinas/Documents/GitHub/CARPS_EXT_29_4_2015/data/Study1 data.sav")

data <- read_sav("data/Study1 data.sav")

```

# Step 3: Tidy data

```{r}
#Data is already tidy
```

# Step 4: Run analysis

## Pre-processing

[you can remove this section if no pre-processing is required]

```{r}
#I don't think I need to do pre-processing 
```

## Descriptive statistics

```{r}
data <- read_sav("data/Study1 data.sav")
pop_mean <- mean("")
```

## Inferential statistics

```{r}
# Compute the analysis of variance
res.aov <- aov(intellect ~ Condition, data = data)
# Summary of the analysis
summary(res.aov)
```

# Step 5: Conclusion

[Please include a text summary describing your findings. If this reproducibility check was a failure, you should note any suggestions as to what you think the likely cause(s) might be.]
  
[PILOT/COPILOT ENTER RELEVANT INFORMATION BELOW]

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

# How many of the above issues were resolved through author assistance?
locus_typo_resolved <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification_resolved <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis_resolved <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data_resolved <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified_resolved <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? TRUE, FALSE, or NA. This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

[PILOT/COPILOT DOD NOT EDIT THE CODE CHUNK BELOW]

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR")) | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified, locus_typo_resolved, locus_specification_resolved, locus_analysis_resolved, locus_data_resolved, locus_unidentified_resolved)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
